# üîß Autograd Implementation from Scratch (using NumPy)

This small project involves implementing **autograd functionality** from scratch using **NumPy** to train a **neural network**. The goal was to build the backpropagation mechanism without relying on libraries like **PyTorch** or **TensorFlow**, providing a deeper understanding of the inner workings of neural network training.

---

## ‚öôÔ∏è Project Details

- **Objective**: Implement autograd for neural network training using only NumPy.
- **Key Focus**: 
  - Backpropagation
  - Automatic differentiation
  - Weight updates using gradient descent

By building this from scratch, the project provides valuable insight into how deep learning frameworks manage gradients and optimize model parameters.

---

## üß† Neural Network

This project covers the implementation of a **basic feedforward neural network** that uses:
- **Sigmoid** activation function
- **Mean Squared Error** loss function

The network is trained using **gradient descent** with the gradients calculated manually.

---

Feel free to explore the code and methodology in this repository, and let me know if you have any questions or suggestions! üöÄ
